\documentclass[a4paper,10pt]{article}

\usepackage{ucs}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{babel}
\usepackage{fontenc}
\usepackage{graphicx}
\usepackage{textgreek}

\usepackage[dvips]{hyperref}

\date{15/05/2019}

\begin{document}
 \section{Storia}
 Uno dei primi esempi di rete neurale fu il percettrone di Rosenbatt (1958). Di seguito fu creata ADELINE (Widrow \& Hoff, 1960), composta da un singolo layer con un singolo neurone e utilizzava il $LMS$ (least mean square) insieme ad un algoritmo di discesa stocastica del gradiente per la configurazione dei pesi. Ne descriviamo adesso le caratteristiche. Siano $x \in R^N$ gli input e $w \in R^N$ i pesi; dal modello abbiamo:
 \begin{equation}
  \widehat{y} = x \cdot w.
 \end{equation}
Dato l'output desiderato $y \in R$ possiamo definire la funzione errore $e$ e la $loss \ function \ L$. L'obiettivo \'e quello di trovare il valore dei pesi che minimizzi $L$.
\begin{align}
 e &= y - \widehat{y}\\
 L &= e^2\\
 w &\leftarrow arg \, min(L).
\end{align}
L'algoritmo $LMS$ ottimizza $L$ usando il gradiente discendente:
\begin{align}
 \nabla L_w: &= -2ex\\
 w &\leftarrow w + \alpha ex
\end{align}
dove $\alpha$ \'e il tasso di apprendimento. 

Widrow, McCool and Ball (1975) hanno esteso l'algoritmo $LMS$ al dominio complesso fornendo la derivazione delle parti reali e immaginarie. Brandwood (1983) generalizz\'o la teoria applicando il gradiente al numero complesso, senza separarlo in parte reale e parte immaginaria attraverso il gradiente di Wirtinger (1927). 

Aggiorniamo quindi il problema nel dominio complesso:
\begin{align}
 L &= e \overline{e}\\
 \nabla L_w : &= -2e\overline{x}\\
 w &\leftarrow w + \alpha e \overline{x}.
\end{align}
Nonostante i risultati di Brandwood, fino a non molto tempo fa la letteratura non applicava il calcolo di Wirtinger, a favore della derivazione separata della parte reale da quella immaginaria. 

\section{Funzione d'attivazione}
Definiamo l'input $x \in C^M$ e i pesi $\textbf{W} \in C^{N\times M}$, con $M$ e $N$ rispettivamente le dimensioni di input e output; di conseguenza l'uotput $y \in C^N$ di un qualsiasi layer \'e determinato da:
\begin{align}
 z = \textbf{W} x\\
 y = f(z)
\end{align}
dove $f$ solitamente \'e una funzione di attivazione non lineare.

\section{Loss function}
La maggior parte della letteratura attuale utilizza come funzione di costo la $mean \, squarred \, error$. Dato un target $y$ e l'output ottenuto $\widehat{y}$, entrambi in $C^N$ e l'errore
\begin{equation}
 e:=y-\widehat{y}\label{error}
\end{equation}
la $complex \, mean \, squared \, loss \, function$ \'e definita come segue:
\begin{align}
 L(e) &= \sum_{i=0}^{N-1} \left| e_i\right|^2\label{lossF}\\
 &=\sum_{i=0}^{N-1} e_i \overline{e_i}.
\end{align}
La \eqref{lossF} \'e una funzione a valori reali scalari non negativi, che tende a zero insieme al modulo dell'errore. Savitha, Suresh e Sundararajan proposero di sostituire l'errore \eqref{error} con:
\begin{equation}
 e:=log \, \widehat{y} - log \, y.\label{complexError}
\end{equation}
La $loss \, function$ diventerebbe quindi:
\begin{equation}
 L(e)=\left( log\left| \widehat{y_i}\right|-log\left| y_i\right|\right)^2 + \left(arg \, \widehat{y_i} - arg \, y_i\right)^2\label{complexLossF}
\end{equation}
L'equazione \eqref{complexLossF} ha la proprietà di rappresentare esplicitamente l'ampiezza e la fase. \\
Le equazioni \eqref{error} e \eqref{complexError} possono essere $error \, function$ appropriate per reti neurali complesse.

\section{Ottimizzazione}
\subsection{Il gradiente complesso ed il calcolo di Wirtinger}
Brandwood e Van den Bos formularono le prime derivazioni del gradiente complesso. Wirtinger (1927) forn\'i un formalismo equivalente che rese il calcolo della derivata di funzioni con valori complessi meno oneroso rispetto a funzioni olomorfe e non analitiche , agendo interamente nel campo complesso. Nonostante tale apparente comodit\'a, solo recentemente si ricominci\'o ad utilizzare il calcolo di Wirtinger per la $backpropagation$ di reti neurali complesse. 

Definiamo:
\begin{align}
 f(z) : &= f\left( z,\overline{z}\right)\\
 &= g\left( x,y\right)\\
 &= u\left( x,y\right) +iv\left( x,y\right)
\end{align}
con $z\in C$, $x,y\in R$ e $z=x+iy$. 

Usando la prima definizione avremo le derivate in $z$ e $\overline{z}$ date da:
\begin{align}
\frac{\partial f}{\partial z}\bigg|_{\overline{z} \ costante}\\
\frac{\partial f}{\partial \overline{z}}\bigg|_{z \ costante}
\end{align}
le quali, espresse in funzioe di $x$ e $y$, diventano:
\begin{align}
 \frac{\partial f}{\partial z} = \frac{1}{2}\left(\frac{\partial f}{\partial x}-i\frac{\partial f}{\partial y}\right)\label{derivatafz}\\
 \frac{\partial f}{\partial \overline{z}} = \frac{1}{2}\left(\frac{\partial f}{\partial x}+i\frac{\partial f}{\partial y}\right).\label{derivatafzcon}
\end{align}
Si osservi che la derivata parziale rispetto a $\overline{z}$ \'e nulla per ogni funzione olomorfa. Richiamiamo le condizioni di esistenza di Cauchy-Riemann per la derivata complessa della funzione $f(z,\overline{z}$ presa in considerazione:
\begin{align}
 \frac{\partial u}{\partial x} &= \frac{\partial v}{\partial y}\\
 \frac{\partial v}{\partial x} &=- \frac{\partial u}{\partial y}.
\end{align}
Se applichiamo le condizioni di Cauchy-Riemann alla derivata di $f$ in $\overline{z}$ \eqref{derivatafzcon} notiamo che effettivamente essa si annulla. Le funzioni olomorfe quindi non dipendono esplicitamente da $\overline{z}$. Brandwood dimostr\'o che l'annullarsi di \eqref{derivatafz} o \eqref{derivatafzcon} per una generica $f:C\rightarrow R$ \'e condizione sufficiente e necessaria affinch\'e $f$ abbia un punto stazionario. Per estensione se $f:C^N\rightarrow R$ \'e una funzione a valori reali di un certo vettore $z=\left[ z_0, \ z_1, \ \cdots \ , \ z_{N-1}\right]^T \in C^N$ e definiamo il cogradiente e il gradiente coniugato come:
\begin{align}
 \frac{\partial}{\partial z} &:=\left[ \frac{\partial}{\partial z_0},\frac{\partial}{\partial z_1},\cdots ,\frac{\partial}{\partial z_{N-1}}\right]\\
 \frac{\partial}{\partial z} &:=\left[ \frac{\partial}{\partial \overline{z_0}},\frac{\partial}{\partial \overline{z_1}},\cdots ,\frac{\partial}{\partial \overline{z_{N-1}}}\right]
\end{align}
allora $\frac{\partial f}{\partial z}=0$ o $\frac{\partial f}{\partial \overline{z}}=0$ sono condizioni sufficienti e necessarie per determinare un punto di stazionariet\'a. 

Se $f$ \'e una funzione di un vettore complesso $z$, la sua derivata totale \'e:
\begin{equation}
 df=\frac{\partial f}{\partial z}dz+\frac{\partial f}{\partial \overline{z}}d\overline{z}.
\end{equation}
Se $f$ \'e reale allora avremmo:
\begin{equation}
 df=2Re\left \{ \frac{\partial f}{\partial z} dz \right \}.
\end{equation}
Definendo ora l'operatore gradiente come:
\begin{align}
 \nabla_z :&= \left(\frac{\partial}{\partial\overline{z}}\right)^T\\
 &=\left(\frac{\partial}{\partial z}\right)^*
\end{align}
pu\'o essere dimostrato, usando la disuguaglianza di Cauchy-Scharz, che $f$ ha il maggior tasso di cambiamento lungo i gradiente. Grazie a queste definizioni possiamo costruire una $cost \, function$ reale con argomenti complessi anche se alcuni elementi della funzione non sono olomorfi. In generale, date le funzioni arbitrarie $f$e $g$, combinando gli jacobiani come segue
\begin{align}
 J_{f\circ g} = J_f J_g + J_f^{\left(c\right)}\overline{\left(J_g^{\left(c\right)}\right)}\\
 J_{f\circ g}^{\left(c\right)} = J_f J_g^{\left(c\right)} + J_f^{\left(c\right)}\overline{\left(J_g\right)}.
\end{align}
Supponiamo di avere una funzione composta $\left(f\circ g\circ h\right)\left(z,\overline{z}\right)$, con $f$ la $cost \, function$ reale, $g$ una funzione complessa non olomorfa e $h$ una funzione olomorfa. tenendo a mente che $f$ \'e una funzione a valori reali di variabile complessa e che $h$ \'e olomorfa ( $\frac{\partial h}{\partial \overline{z} = 0}$ ), applichiamo la $chain \ rule$:
\begin{align}
 J_f &= \frac{ \partial f}{ \partial g}\\
 J_f^{\left( c\right)} &= \frac{\partial f}{\partial \overline{g}}\\
 J_{f\circ g} &= J_f \frac{\partial g}{\partial h} + J_f^{\left( c\right)} \overline{\left( \frac{\partial g}{\partial \overline{h}}\right) }\\
 J_{f\circ g\circ h} &= J_{f\circ g} \frac{\partial h}{\partial z}\\
 \nabla_z f &= \left( J_{f\circ g\circ h}\right)^*
\end{align}
Il Calcolo di Wirtinger rende un po' pi\'u semplice costruire un grafico computazionale per reti complesse aventi composizioni miste di operazioni olomorfe e non olomorfe.

\subsection{Inputs}
La funzione che la rete neurale deve apprendere ha come argomenti i vettori reali $\alpha$ e $\varphi$ che rappresentano rispettivamente le componenti di ampiezza e fase. Di seguito mostriamo alcuno modalità per rappresentare l'input:
\begin{description}
 \item[Ampiezza-Fase]
 I parametri di ampiezza e fase vengono concatenati nel vettore reale:
 \begin{equation}
  x^{\left( ap\right) } :=\left[ \cdots, \ \alpha_i, \ \varphi_i, \ \cdots\right]^T\in R^{2K}
 \end{equation}

 \item[Complessa] 
 I parametri vengono rappresentati all'interno del vettore complesso:
 \begin{equation}
  x^{\left( c\right) } :=\left[ \cdots, \ \alpha_i e^{i \varphi_i}, \ \cdots\right]^T\in C^{K}\label{inputComp}
 \end{equation}
 
 \item[Reale-Immaginaria]
 Possiamo scomporre il vettore \eqref{inputComp} in parte reale e parte immaginaria e ottenere un input nella forma:
 \begin{align}
  x^{\left( ri\right) } :&=\left[ Re\left \{ x^{\left(c\right) }\right \}^T, \ Im\left \{ x^{\left(c\right) } \right \} \right]^T\\
  &=\left[ \cdots \ \alpha_i cos\varphi_i \ \cdots \ \alpha_i sin\varphi_i \ \cdots\right]^T
 \end{align}

 \item[Complesso aumentata]
 L'input \'e un vettore complesso contenente sia \eqref{inputComp} che il suo coniugato:
 \begin{align}
  x^{\left( ca\right) } :&=\left[ \left( x^{\left(c\right) }\right)^T, \ \left( x^{\left(c\right) } \right)^* \right]^T\\
  &=\left[ \cdots, \ \alpha_i e^{i \varphi_i} \ \cdots \ \alpha_i e^{-i \varphi_i} \ \cdots\right]^T
 \end{align}

 Il $mapping$ definito dal complesso coniugato \'e antilineare e di conseguenza non pu\'o essere calcolato attraverso la moltiplicazione di matrici complesse. ci\'o potrebbe richiedere alla rete l'apprendimento di un ulteriore parametro. In effetti. l'utilizzo del vettore complesso aumentato fornisce un $path$ pi\'u semplice per la modellizzazione della distribuzione completa al secondo ordine dei dati. Gli inut aumentati sono utilizzati in una stima lineare.
\end{description}

\subsection{Architettura}
Esaminiamo un modello $feedforward$ avente un unico hidden layer; abbiamo di conseguenza:
\begin{align}
 h &= f\left(W^{\left(h\right)}x+b^{\left(h\right)}\right)\\
 \widehat{y} &= W^{\left(o\right)}x+b^{\left(o\right)}
\end{align}
con $\theta = \left \{ W^{\left(h\right)}, \ W^{\left(o\right)}, \ b^{\left(h\right)}, \ b^{\left(o\right)}\right \}$ sono i parametri del modello. Definendo $M$ il numero dei nodi dell'input e $N$ il numero dei nodi dell'output, avremo:
\begin{equation}
 \begin{matrix}
  W^{\left( h\right) } \in R^{M\times M} & or & W^{\left( o\right) } \in C^{M\times M}\\
  b^{\left( h\right) } \in R^M & or & b^{\left( o\right) } \in C^M\\
  W^{\left( o\right) } \in R^{N\times M} & or & W^{\left( o\right) } \in C^{N\times M}\\
  b^{\left( o\right) } \in R^N & or & b^{\left( o\right) } \in C^N
 \end{matrix}
\end{equation}
In tutti i casi gli inut, i pesi e gli output appartengono allo stesso dominio numerico.

\subsection{Funzioni di attivazione}
\begin{description}
 \item[Identit\'a]
 Permette una modellizazione lineare
 \begin{equation}
  f^{\left(-\right) }\left(x\right) := x
 \end{equation}

 \item[Tangente Iperbolica]
 E\' una funzione sigmoidale e differenziabile. Permette una non linearit\'a ampiamente utilizzata per l'apprendimento delle reti neurali
 \begin{equation}
  f^{\left(\sigma\right) } := tanh\left( x\right)
 \end{equation}

 \item[Split reale-immaginario]
 Applica la tangente iperbolica separatamente alla parte reale e alla parte immaginaria:
 \begin{equation}
  f^{\left( ri\right)} \left( x\right) := tanh\left( Re \ x\right) +i \ tanh\left( Im \ x\right)
 \end{equation}

 \item[Split ampiezza-fase]
 Applica la tangente iperbolica al modulo del numero complesso, senza modificarne la fase (questa funzione non \'e differenziabile nel campo complesso)
 \begin{equation}
  f^{\left( ap\right) } \left( x\right) := tanh\left( \left| x\right|\right) e^{i \, arg \, x}
 \end{equation}
 
 \item[ModReLU]
 Arjosky nel 2015 propose una variazione alla classica ReLU utilizzata nelle reti neurali reali, definita come segue:
 \begin{equation}
  ModReLU\left( z\right) = ReLU\left( \left| z\right|+b\right) e^{i\theta_z} = \begin{cases}
                                                                                               \left(\left| z\right| +b\right) \frac{z}{\left| z\right|} & \mbox{se} \left| z\right| +b \ge 0 \\
                                                                                0 & \mbox{altrimenti}
                                                                
                                                                \end{cases}
 \end{equation}
 dove $\theta_z$ \'e la fase di $z$ e $b$ il bias, apprendibile dalla rete neurale, e necessario per creare una $dead \ zone$ di raggio b attorno all'origine dove il neurone \'e inattivo. Tale funzione non soddisfa le equazioni di Cauchy-Riemann e quindi non \'e olomorfa.
 
 \item[CReLU]
 Consiste in una applicazione della funzione ReLu individualmente alla parte reale e alla parte immaginaria:
 \begin{equation}
  CReLU\left( z\right) = ReLU\left( Re\left( z\right)\right) + i \, ReLU\left( Im\left( z\right)\right).
 \end{equation}
 Questa soddisfa le condizioni di Cauchy-Riemann solo se sia la parte reale che la parte immaginaria sono contemporaneamente strettamente positive o strettamente negative, quindi nell'intervallo $\theta_z \in \left( 0, \frac{\pi}{2}\right)$ oppure $\theta_z \in \left( \pi, \frac{ 3 \pi}{2}\right)$.
 
 \item[zReLU]
 Proposta nel 2016 da Guberman e basata anch'essa sulla ReLu, \'e definita come:
 \begin{equation}
   zReLU\left( z\right) = \begin{cases}
                                                                                               z & \mbox{se} \theta_z \in \left[ 0, \frac{ \pi}{2}\right] \\
                                                                                0 & \mbox{altrimenti}
                                                             
                                                     \end{cases}
 \end{equation}




\end{description}



\end{document}
