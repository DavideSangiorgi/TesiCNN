\documentclass[a4paper,10pt]{article}

\usepackage{ucs}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{babel}
\usepackage{fontenc}
\usepackage{graphicx}
\usepackage{textgreek}
\usepackage{subcaption}

\usepackage{hyperref}

\date{12/08/2019}

\begin{document}
 
 \section{Abstract}
 
 L'idea di base \'e che un effetto sinergico all'interno dell'ensemble migliora la performance e la tolleranza agli errori. 
 
 
 \section{Reti neurali}
 \subsection{Storia}
 
 \subsection{Struttura}
 
 \subsection{Funzione d'attivazione}
 
 \subsection{Loss function}
 
 \subsection{Back propagation}
 
 \section{Reti neurali a valori complessi}
 \subsection{Storia}
 Uno dei primi esempi di rete neurale fu il percettrone di Rosenbatt (1958). Di seguito fu creata ADELINE (Widrow \& Hoff, 1960), composta da un singolo layer con un singolo neurone e utilizzava il $LMS$ (least mean square) insieme ad un algoritmo di discesa stocastica del gradiente per la configurazione dei pesi. Ne descriviamo adesso le caratteristiche. Siano $x \in R^N$ gli input e $w \in R^N$ i pesi; dal modello abbiamo:
 \begin{equation}
  \widehat{y} = x \cdot w.
 \end{equation}
 Dato l'output desiderato $y \in R$ possiamo definire la funzione errore $e$ e la $loss \ function \ L$. L'obiettivo \'e quello di trovare il valore dei pesi che minimizzi $L$.
 \begin{align}
  e &= y - \widehat{y}\\
  L &= e^2\\
  w &\leftarrow arg \, min(L).
 \end{align}
 L'algoritmo $LMS$ ottimizza $L$ usando il gradiente discendente:
 \begin{align}
  \nabla L_w: &= -2ex\\
  w &\leftarrow w + \alpha ex
 \end{align}
 dove $\alpha$ \'e il tasso di apprendimento. 

 Widrow, McCool and Ball (1975) hanno esteso l'algoritmo $LMS$ al dominio complesso fornendo la derivazione delle parti reali e immaginarie. Brandwood (1983) generalizz\'o la teoria applicando il gradiente al numero complesso, senza separarlo in parte reale e parte immaginaria attraverso il gradiente di Wirtinger (1927). 

 Aggiorniamo quindi il problema nel dominio complesso:
 \begin{align}
  L &= e \overline{e}\\
  \nabla L_w : &= -2e\overline{x}\\
  w &\leftarrow w + \alpha e \overline{x}.
 \end{align}
 Nonostante i risultati di Brandwood, fino a non molto tempo fa la letteratura non applicava il calcolo di Wirtinger, a favore della derivazione separata della parte reale da quella immaginaria. 
 
 \subsection{Struttura}
 Esaminiamo un modello $feedforward$ avente un unico hidden layer; abbiamo di conseguenza:
 \begin{align}
  h &= f\left(W^{\left(h\right)}x+b^{\left(h\right)}\right)\\
  \widehat{y} &= W^{\left(o\right)}x+b^{\left(o\right)}
 \end{align}
 con $\theta = \left \{ W^{\left(h\right)}, \ W^{\left(o\right)}, \ b^{\left(h\right)}, \ b^{\left(o\right)}\right \}$ sono i parametri del modello. Definendo $M$ il numero dei nodi dell'input e $N$ il numero dei nodi dell'output, avremo:
 \begin{equation}
  \begin{matrix}
   W^{\left( h\right) } \in R^{M\times M} & or & W^{\left( o\right) } \in C^{M\times M}\\
   b^{\left( h\right) } \in R^M & or & b^{\left( o\right) } \in C^M\\
   W^{\left( o\right) } \in R^{N\times M} & or & W^{\left( o\right) } \in C^{N\times M}\\
   b^{\left( o\right) } \in R^N & or & b^{\left( o\right) } \in C^N
  \end{matrix}
 \end{equation}
 In tutti i casi gli inut, i pesi e gli output appartengono allo stesso dominio numerico.
 
 \subsection{Funzione d'attivazione}
 
 \begin{description}
 \item[Identit\'a]
 Permette una modellizazione lineare
 \begin{equation}
  f^{\left(-\right) }\left(x\right) := x
 \end{equation}

 \item[Tangente Iperbolica]
 E\' una funzione sigmoidale e differenziabile. Permette una non linearit\'a ampiamente utilizzata per l'apprendimento delle reti neurali
 \begin{equation}
  f^{\left(\sigma\right) } := tanh\left( x\right)
 \end{equation}

 \item[Split reale-immaginario]
 Applica la tangente iperbolica separatamente alla parte reale e alla parte immaginaria:
 \begin{equation}
  f^{\left( ri\right)} \left( x\right) := tanh\left( Re \ x\right) +i \ tanh\left( Im \ x\right)
 \end{equation}

 \item[Split ampiezza-fase]
 Applica la tangente iperbolica al modulo del numero complesso, senza modificarne la fase (questa funzione non \'e differenziabile nel campo complesso)
 \begin{equation}
  f^{\left( ap\right) } \left( x\right) := tanh\left( \left| x\right|\right) e^{i \, arg \, x}
 \end{equation}
 
 \item[ModReLU]
 Arjosky nel 2015 propose una variazione alla classica ReLU utilizzata nelle reti neurali reali, definita come segue:
 \begin{equation}
  ModReLU\left( z\right) = ReLU\left( \left| z\right|+b\right) e^{i\theta_z} = \begin{cases}
                                                                                               \left(\left| z\right| +b\right) \frac{z}{\left| z\right|} & \mbox{se} \left| z\right| +b \ge 0 \\
                                                                                0 & \mbox{altrimenti}
                                                                
                                                                \end{cases}
 \end{equation}
 dove $\theta_z$ \'e la fase di $z$ e $b$ il bias, apprendibile dalla rete neurale, e necessario per creare una $dead \ zone$ di raggio b attorno all'origine dove il neurone \'e inattivo. Tale funzione non soddisfa le equazioni di Cauchy-Riemann e quindi non \'e olomorfa.
 
 \item[CReLU]
 Consiste in una applicazione della funzione ReLu individualmente alla parte reale e alla parte immaginaria:
 \begin{equation}
  CReLU\left( z\right) = ReLU\left( Re\left( z\right)\right) + i \, ReLU\left( Im\left( z\right)\right).
 \end{equation}
 Questa soddisfa le condizioni di Cauchy-Riemann solo se sia la parte reale che la parte immaginaria sono contemporaneamente strettamente positive o strettamente negative, quindi nell'intervallo $\theta_z \in \left( 0, \frac{\pi}{2}\right)$ oppure $\theta_z \in \left( \pi, \frac{ 3 \pi}{2}\right)$.
 
 \item[zReLU]
 Proposta nel 2016 da Guberman e basata anch'essa sulla ReLu, \'e definita come:
 \begin{equation}
   zReLU\left( z\right) = \begin{cases}
                                                                                               z & \mbox{se} \theta_z \in \left[ 0, \frac{ \pi}{2}\right] \\
                                                                                0 & \mbox{altrimenti}
                                                             
                                                     \end{cases}
 \end{equation}

\end{description}
 
 \subsection{Back propagation}
 
 \subsubsection{Loss function}
 La maggior parte della letteratura attuale utilizza come funzione di costo la $mean$ $squarred$ $error$. Dato un target $y$ e l'output ottenuto $\widehat{y}$, entrambi in $C^N$ e l'errore: 
 \begin{equation}
  e:=y-\widehat{y}\label{error}
 \end{equation}
 la $complex$ $mean$ $squared$ $loss$ $function$ \'e definita come segue:
 \begin{align}
  L(e) &= \sum_{i=0}^{N-1} \left| e_i\right|^2\label{lossF}\\
  &=\sum_{i=0}^{N-1} e_i \overline{e_i}.
 \end{align}
 La \ref{lossF} \'e una funzione a valori reali scalari non negativi, che tende a zero insieme al modulo dell'errore. Savitha, Suresh e Sundararajan proposero di sostituire l'errore \ref{error} con:
 \begin{equation}
  e:=log ( \widehat{y} ) - log ( y ).\label{complexError}
 \end{equation}
 La $loss$ $function$ diventerebbe quindi:
 \begin{equation}
  L(e)=\left( log\left| \widehat{y_i}\right|-log\left| y_i\right|\right)^2 + \left(arg( \widehat{y_i}) - arg  (y_i)\right)^2 \label{complexLossF}
 \end{equation}
 L'equazione \ref{complexLossF} ha la propriet\'a di rappresentare esplicitamente l'ampietta e la fase.
 Le equazioni \ref{error} e \ref{complexError} possono essere $error$ $function$ appropriate per reti neurali complesse.
 
 \subsubsection{Il gradiente complesso ed il calcolo di Wirtinger}
 Brandwood e Van den Bos formularono le prime derivazioni del gradiente complesso. Wirtinger (1927) forn\'i un formalismo equivalente che rese il calcolo della derivata di funzioni con valori complessi meno oneroso rispetto a funzioni olomorfe e non analitiche , agendo interamente nel campo complesso. Nonostante tale apparente comodit\'a, solo recentemente si ricominci\'o ad utilizzare il calcolo di Wirtinger per la $backpropagation$ di reti neurali complesse. 

 Definiamo:
 \begin{align}
  f(z) : &= f\left( z,\overline{z}\right)\\
  &= g\left( x,y\right)\\
  &= u\left( x,y\right) +iv\left( x,y\right)
 \end{align}
 con $z\in C$, $x,y\in R$ e $z=x+iy$. 

 Usando la prima definizione avremo le derivate in $z$ e $\overline{z}$ date da:
 \begin{align}
  \frac{\partial f}{\partial z}\bigg|_{\overline{z} \ costante}\\
  \frac{\partial f}{\partial \overline{z}}\bigg|_{z \ costante}
 \end{align}
 le quali, espresse in funzioe di $x$ e $y$, diventano:
 \begin{align}
  \frac{\partial f}{\partial z} = \frac{1}{2}\left(\frac{\partial f}{\partial x}-i\frac{\partial f}{\partial y}\right)\label{derivatafz}\\
  \frac{\partial f}{\partial \overline{z}} = \frac{1}{2}\left(\frac{\partial f}{\partial x}+i\frac{\partial f}{\partial y}\right).\label{derivatafzcon}
 \end{align}
 Si osservi che la derivata parziale rispetto a $\overline{z}$ \'e nulla per ogni funzione olomorfa. Richiamiamo le condizioni di esistenza di Cauchy-Riemann per la derivata complessa della funzione $f(z,\overline{z}$ presa in considerazione:
 \begin{align}
  \frac{\partial u}{\partial x} &= \frac{\partial v}{\partial y}\\
  \frac{\partial v}{\partial x} &=- \frac{\partial u}{\partial y}.
 \end{align}
 Se applichiamo le condizioni di Cauchy-Riemann alla derivata di $f$ in $\overline{z}$ \ref{derivatafzcon} notiamo che effettivamente essa si annulla. Le funzioni olomorfe quindi non dipendono esplicitamente da $\overline{z}$. Brandwood dimostr\'o che l'annullarsi di \ref{derivatafz} o \ref{derivatafzcon} per una generica $f:C\rightarrow R$ \'e condizione sufficiente e necessaria affinch\'e $f$ abbia un punto stazionario. Per estensione se $f:C^N\rightarrow R$ \'e una funzione a valori reali di un certo vettore $z=\left[ z_0, \ z_1, \ \cdots \ , \ z_{N-1}\right]^T \in C^N$ e definiamo il cogradiente e il gradiente coniugato come:
 \begin{align}
  \frac{\partial}{\partial z} &:=\left[ \frac{\partial}{\partial z_0},\frac{\partial}{\partial z_1},\cdots ,\frac{\partial}{\partial z_{N-1}}\right]\\
  \frac{\partial}{\partial z} &:=\left[ \frac{\partial}{\partial \overline{z_0}},\frac{\partial}{\partial \overline{z_1}},\cdots ,\frac{\partial}{\partial \overline{z_{N-1}}}\right]
 \end{align}
 allora $\frac{\partial f}{\partial z}=0$ o $\frac{\partial f}{\partial \overline{z}}=0$ sono condizioni sufficienti e necessarie per determinare un punto di stazionariet\'a. 

 Se $f$ \'e una funzione di un vettore complesso $z$, la sua derivata totale \'e:
 \begin{equation}
  df=\frac{\partial f}{\partial z}dz+\frac{\partial f}{\partial \overline{z}}d\overline{z}.
 \end{equation}
 Se $f$ \'e reale allora avremmo:
 \begin{equation}
  df=2Re\left \{ \frac{\partial f}{\partial z} dz \right \}.
 \end{equation}
 Definendo ora l'operatore gradiente come:
 \begin{align}
  \nabla_z :&= \left(\frac{\partial}{\partial\overline{z}}\right)^T\\
  &=\left(\frac{\partial}{\partial z}\right)^*
 \end{align}
 pu\'o essere dimostrato, usando la disuguaglianza di Cauchy-Scharz, che $f$ ha il maggior tasso di cambiamento lungo i gradiente. Grazie a queste definizioni possiamo costruire una $cost \, function$ reale con argomenti complessi anche se alcuni elementi della funzione non sono olomorfi. In generale, date le funzioni arbitrarie $f$e $g$, combinando gli jacobiani come segue
 \begin{align}
  J_{f\circ g} = J_f J_g + J_f^{\left(c\right)}\overline{\left(J_g^{\left(c\right)}\right)}\\
  J_{f\circ g}^{\left(c\right)} = J_f J_g^{\left(c\right)} + J_f^{\left(c\right)}\overline{\left(J_g\right)}.
 \end{align}
 Supponiamo di avere una funzione composta $\left(f\circ g\circ h\right)\left(z,\overline{z}\right)$, con $f$ la $cost \, function$ reale, $g$ una funzione complessa non olomorfa e $h$ una funzione olomorfa. tenendo a mente che $f$ \'e una funzione a valori reali di variabile complessa e che $h$ \'e olomorfa ( $\frac{\partial h}{\partial \overline{z} = 0}$ ), applichiamo la $chain \ rule$:
 \begin{align}
  J_f &= \frac{ \partial f}{ \partial g}\\
  J_f^{\left( c\right)} &= \frac{\partial f}{\partial \overline{g}}\\
  J_{f\circ g} &= J_f \frac{\partial g}{\partial h} + J_f^{\left( c\right)} \overline{\left( \frac{\partial g}{\partial \overline{h}}\right) }\\
  J_{f\circ g\circ h} &= J_{f\circ g} \frac{\partial h}{\partial z}\\
  \nabla_z f &= \left( J_{f\circ g\circ h}\right)^*
 \end{align}
 Il Calcolo di Wirtinger rende un po' pi\'u semplice costruire un grafico computazionale per reti complesse aventi composizioni miste di operazioni olomorfe e non olomorfe.
 
 \subsection{Confronto con rete neurale}
 
 \section{Ottimizzazione}
 \subsection{ensemble di reti neurali}
 Una rete di dimensioni finite raramente apprende completamente un particolare mapping e pu\'o generalizzare male. Aumentare la dimensione o il numero di hidden layer per\'o, il pi\'u dele volte, non porta a nessun miglioramento (Soulie, 1987) anzi, aumentando la dimensionalit\'a del problema aumenta la possibilit\'a che la rete si stabilizzi lontana dalla soluzione desiderata. 
 Molti ricercatori hanno dimostrato che la semplice combinazione degli output di molti classificatori pu\'o generare previsioni pi\'u accurate di quelle di qualsiasi classificatore (Clemen 1989; Wolpert 1992). In particolare, la combinazione di reti neurali addestrate separatamente (comunemente indicato come ensemble di reti neurali) si \'e dimostrata particolarmente efficace (Alpaydin 1993; Drucker et al. 1994; Krogh e Vedelsby 1995; Maclin e Shavlik 1995; Perrone 1992). 
 Figura \ref{EnsembleStructurepng} illustra la struttura generale di un ensemble di reti neurali. Ogni rete nell'ensemble \'e per prima cosa addestrato e poi, per ogni esempio del training, l'output predetto di ognuna delle reti \'e combinato dal judge per produrre l'output dell'ensemble. 
 L'output dell'ensemble pu\'o anche essere utilizzato come ulteriore training restituendolo alle singole reti. Infatti, nel caso in cui nell'ensemble vi sia una o pi\'u reti poco efficienti e non si hanno a disposizione pi\'u esempi per il training, si pu\'o utilizzare l'output dell'ensemble come output desiderato nelle seingole reti per continuare l'addestramento (aggiungere riferimento). 
 L'errore di una singola rete dipende dalle dimensioni del hidden layer e dalla durata del training. Tuttavia in generale l'errore non \'e una funzione decrescente della dimensione dell'hidden layer. In figura \ref{NetErrHiddenLa} e \ref{NetErrHiddenLb} si mostra l'errore di una singola rete rispetto alla dimensione dell'hidden layer per diversi step di training. D'altra parte la miglior capacit\'a di apprendimento di un ensemble dipende, come accennato in precedenza, dalla sinergia tra le singole reti e non dalle maggiori dimensioni rispetto alla singola rete. Questo \'e principalmente ci\'o di cui discutono anche William P. Lincoln e Josef Skrzypek in $Synergy$ $of$ $clustering$ $multiple$ $back$ $propagation$ $networks$, dove confrontano le prestazioni di ogni singola rete con le prestazioni di un enseble di 5 reti neurali, con stessa struttura della singola.
 
 \begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.4\linewidth}
   \includegraphics[width=\linewidth]{NetErrHiddenLa.png}
   \caption{da inserire}
   \label{NetErrHiddLapgn}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\linewidth}
   \includegraphics[width=\linewidth]{NetErrHiddenLb.png}
   \caption{da inserire}
   \label{NetErrHiddLbpgn}
  \end{subfigure}
 \end{figure}

 
 Vengono confrontati gli errori medi dei due sistemi. Una misura utile del vantaggio dell'ensemble si ottiene prendendo il rapporto tra l'errore di una singola rete e l'errore del judge. Questo rapporto sar\'a inferiore o maggiore di 1 a seconda del valore assoluto dell'errore della singola rete neurale e dell'ensemble. Figura \ref{EnsembleAdvantagepng} mostra il vantaggio dell'ensemble rispetto agli errori della rete singola rispettivamente per 256 e 1024 step del training. Nei casi estremi in cui l'apprendimento sia nullo o quasi totale, l'ensemble non mostra particolari vantaggi, tuttavia si ha un notevole vantaggio se l'apprendimento \'e solo parziale.
 
 \begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.4\linewidth}
   \includegraphics[width=\linewidth]{ClusterErrora.png}
   \caption{da inserire}
   \label{ClusterErrorapgn}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\linewidth}
   \includegraphics[width=\linewidth]{ClusterErrorb.png}
   \caption{da inserire}
   \label{ClusterErrorbpgn}
  \end{subfigure}
 \end{figure}
 
 \subsubsection{judge}
 
 Dato un singolo input, le reti neurali dell'ensemble daranno presumibilmente degli output $Y_k$ diversi, che diventano l'input del $judge$. Quest'ultimo combinandoli ottiene il risultato finale $Y$. Esistono diversi metodi per calcolare $Y$, ma per semplicit\'a consideriamo i due casi pi\'u semplici:
 \begin{align}
  Y &= \sum_{k=1}^N \frac{1}{N} Y_k \\
  Y &= \sum_{k=1}^N W_k Y_k
 \end{align}
 Il primo \'e dato da una semplice media dei $Y_k$, mentre nel secondo caso si considera una media pesata, con i $W_k$ che indicano ``l'affidabilit\'a'' delle singole reti neurali. Seguendo questo ragionamento i $W_k$ vengono di volta in volta modificati come segue:
 \begin{align}
  W_k &= W_k \cdot G \cdot \frac{e}{e_k} \label{JudgeWeights} \\
  e &= \frac{1}{N} \sum{k=1}^N e_k \\
  e_k &= \left | y - y_k \right |
 \end{align}
 Con $e$ che indica il tasso di correzione e $e_k$ \'e la deviazione dell'output della singola rete dall'output dell'ensemble.
 
 Dopo il periodo di training iniziale, si presume generalmente che la rete neurale non sia pi\'u soggetta ad ulteriore training. Tuttavia, si pu\'o restituire l'output del judge alle singole reti neurali come output desiderato. William P. Lincoln e Josef Skrzypek in $Synergy$ $of$ $clustering$ $multiple$ $back$ $propagation$ $networks$ affermano che questo procedimento migliori la resistenza al rumore e l'auto-organizzazione $(da capire cosa sia)$.
  
 \subsubsection{ottimizzazione dell'ensemble}
  Precedenti lavori sia teorici (Hansen e Salamon 1990; Krogh e Vedelsby 1995) che empirici (Hashem et al. 1994; Maclin e shavik 1995) hanno dimostrato che un ensemble efficace dovrebbe consistere non solo in reti particolarmente performanti, ma anche in reti che commettano errori in parti distinte dello spazio degli input. 
  Assumiamo che l'obiettivo sia quello di apprendere una funzione $f:R^N\to R$ per la quale abbiamo $p$ coppie input-output per il training con $Y_k = f \left( x_k \right) $ e $k=1, \cdots , p$. L'ensenble consiste in $N$ reti e chiamiamo $V^{\alpha}\left( x\right)$ l'output della rete $\alpha$ relativo all'input $x$. Definiamo nel modo seguente il valore ottenuto dal judge: 
  \begin{equation}
   \overline{V}\left(x\right) = \sum_{\alpha} w_{\alpha} V^{\alpha}\left(x\right).
  \end{equation}
  Consideriamo i pesi $w_{\alpha}$ come l'affidabilit\'a della rete $\alpha$ e di conseguenza li vincoliamo ad essere positivi e normalizzati, cio\'e $\sum_{\alpha} w_{\alpha} = 1$. L'\textit{ambuiguit\'a} su un input $x$ di un singolo membro dell'ensemble \'e definito come $a^{\alpha}\left(x\right)=\left(V^{\alpha}\left(x\right)-\overline{V}\left(x\right)\right)^2$. La \textit{ambiguit\'a dell'ensemble} su un input $x$ \'e data da:
  \begin{equation}
   \overline{a}\left(x\right) = \sum_{\alpha} w_{\alpha} a^{\alpha}\left(x\right) = \sum_{\alpha} w_{\alpha}\left(V^{\alpha}\left(x\right)-\overline{V}\left(x\right)\right)^2. \label{EnsembleAmbiguity}
  \end{equation}
  \'E quindi una varianza e misura il disaccordo delle reti sull'input $x$. L'errore quadratico della rete $\alpha$ e dell'ensemble  sono rispettivamente:
  \begin{align}
   e^{\alpha}\left(x\right) &= \left(f\left(x\right)-V^{\alpha}\left(x\right)\right)^2 \\
   e\left(x\right) &= \left(f\left(x\right)-\overline{V}\left(x\right)\right)^2 \label{ensembleQE}
  \end{align}
  Aggiungendo e sottraendo $f\left(x\right)$ alla \ref{EnsembleAmbiguity} si ottiene:
  \begin{equation}
   \overline{a}\left(x\right) = \sum_{\alpha} w_{\alpha} e^{\alpha}\left(x\right) - e\left(x\right).
  \end{equation}
  Chiamando la media pesata degli errori delle singole reti $\overline{e}\left(x\right) = \sum_{\alpha} w_{\alpha} e^{\alpha}\left(x\right)$ allora la \ref{ensembleQE} diventa:
  \begin{equation}
   e\left(x\right) = \overline{e}\left(x\right) - \overline{a}\left(x\right). \label{GeneralizationError}
  \end{equation}
  Queste ultime formule possono essere mediate lungo la distribuzione degli input, ottenendo le seguenti:
  \begin{align}
   E^{\alpha} &= \int dx \; p\left(x\right) \; e^{\alpha}\left(x\right) \\
   A^{\alpha} &= \int dx \; p\left(x\right) \; a^{\alpha}\left(x\right) \\
   E &= \int dx \; p\left(x\right) \; e\left(x\right).
  \end{align}
  Le prime due sono rispettivamente l'errore generalizzato e l'ambiguit\'a della rete $\alpha$ e $E$ \'e l'errore generalizzato dell'ensemble. Dalla \ref{GeneralizationError} otteniamo:
  \begin{equation}
   E = \overline{E} - \overline{A} \label{EnsembleGeneralizationError}
  \end{equation}
  con $\overline{E} = \sum_{\alpha} w_{\alpha} E^{\alpha}$ la media pesata degli errori generalizzati delle singole reti e $\overline{A} = \sum_{\alpha} w_{\alpha} A^{\alpha}$ la media pesata delle ambiguit\'a delle singole reti.
  Questa equazione separa l'errore generalizzato in un termine che dipende dagli errori generalizzati delle singole reti e da un altro contenente le correlazioni tra le reti. Inoltre, il termine di correlazione $A$ pu\'o essere stimato interamente da dati non etichettati, non \'e richiesta alcuna conoscenza della funzione da approssimare. Il termine ``senza etichetta'' \'e tratto dai problemi di classificazione e in questo contesto si riferisce a un input $x$ per il quale non si conosce il valore $f(X)$ della funzione target. 
  Se l'ensemble \'e fortemente distorto, l'ambiguit\'a sar\'a piccola, perch\'e le reti implementano funzioni molto simili e concordano quindi gli input anche al di fuori del training. Pertanto l'errore generalizzato sar\'a sostanzialmente uguale alla media degli errori delle singole reti. Se, d'altra parte, c'\'e una grande varianza, l'ambiguit\'a \'e alta e in questo caso l'errore di generalizzazione sar\'a pi\'u piccolo dell'errore di generalizzazione medio.
  Vediamo immediatamente che l'errore generalizzato dell'ensemble \'e sempre minore della media pesata degli errori degli ensemble: $E < \overline{E}$. In particolare, per pesi uniformi:
  \begin{equation}
   E \le \frac{1}{N} \sum_{\alpha} E^{\alpha}
  \end{equation}
  Dalla \ref{EnsembleGeneralizationError} si ricava che aumentare l'efficienza dell'ensemble significa aumentare l'ambiguit\'a e quindi la discordanza tra le reti singole senza aumentarne ovviamente l'errore generalizzato. Come aumentare l'ambiguit\'a dell'ensemble? Un metodo pu\'o essere quello di utilizare diverse tipologie di reti neurali, oppure si utilizzano set di training diversi. Inoltre, per essere in grado di stimare il primo termine in \ref{EnsembleGeneralizationError}, sarebbe auspicabile una cross-validation. William P. Lincoln e Josef Skrzypek in $Synergy of clustering multiple back propagation networks$ testarono questo metodo per approssimare con un ensemble di reti neurali un'onda quadra in una variabile, mostrata in figura \ref{SqWavepng}. Sono state utilizzate 5 reti con un hidden layer di 20 ``neuroni'', addestrate indipendenemente le une dalle altre mediande back-propagation utilizzando 200 esempi casuali. La ``vera'' generalizzazione e l'ambiguit\'a sono state stimate da un insieme di 1000 input e i pesi associati agli output delle singole reti per la combinazione erano uniformi $w_{\alpha} = 1/5$.
  \begin{figure}[h!]
   \centering
   \includegraphics[scale=0.4]{SqWave.png}
   \caption{da inserire}
   \label{SqWavepng}
  \end{figure}
  In figura \ref{GenErrorpng} viene mostrato l'errore generalizzato in funzione della dimensione $K$ dei set per la cross-validation.
  \begin{figure}[h!]
   \centering
   \includegraphics[scale=0.4]{GenError.png} 
   \caption{da inserire}
   \label{GenErrorpng}
  \end{figure}
  Innanzitutto notiamo come l'errore generalizzato sia lo stesso per un set di cross-validation di dimensione 40 o 0. Tuttavia bisogna considerare che sono stati scartati tutti i risultati di ensemble due o pi\'u reti non convergenti.

  Dopo aver definito pi\'u chiaramente il problema da affrontare, cerchiamone una soluzione. La maggior parte dei lavori precedenti si \'e concentrata sulla combinazione degli output di reti pi\'u performanti o ha indirizzato solo indirettamente il modo in cui generare un buon ensemble di reti, generando casualmente diverse topologie, impostazioni iniziali dei pesi, dei parametri o utilizzando solo una parte del set di training nella speranza di produrre reti che commettano errori per input distinti. Per trovare direttamente un ensemble accurato e diversificato si possono sfruttare gli algoritmi genetici, creando una popolazione iniziale e utilizzando operatori genetici per creare continuamente nuove reti, mantenendo di volta in volta solo l'insieme di reti pi\'u performanti e al contempo pi\'u discordi tra loro. 

  
  In tabella 1 sintetizziamo l'agoritmo utilizzato. Viene creata una popolazione iniziale di reti neurali sottoposta a training. Di seguito si creano nuove reti partendo dalle precedenti utilizzando operazioni ``genetiche'' come mutazioni e crossover. La nuova popolazione viene nuovamente addestrata, ponendo particolare attenzione ad utilizzare gli esempi classificati erroneamente dalla popolazione precedente. Viene quindi associato un punteggio determinato dalla funzione di fitness:
  \begin{equation}
   Fitness_i = Accuracy_i + \lambda Diversity_i = (1-E_i) + \lambda D_i \label{fitness}
  \end{equation}
  con la diversit\'a definita come:
  \begin{equation}
   D_i = \sum \left[ V_i \left(x\right) - \overline{V} \left(x\right) \right]^2. \label{diversity}
  \end{equation}
  
  Definiamo il termine di accuratezza come $A_i = 1-E_i$ come accuratezza del set di convalida della rete e utilizziamo l'equazione \ref{diversity} per calcolare il termine di diversit\'a $D_i$. Quindi normalizziamo separatamente ciascuno dei due termini. Normalizzare entrambi i termini permette a $\lambda$ di mantenere o stesso significato attraverso i domini. Non essendo sempre chiero con quale valore si debba settare $\lambda$, solitamente ci si basa sulle seguenti regole. Il valore di $\lambda$ non si modifica se l'errore dell'ensemble $\hat{E}$ diminuisce mentre consideriamo nuove reti. Cambia invece se si verifica una delle seguenti: (1) se l'errore della popolazione $\overline{E}$ non sta aumentando e la diversit\'a $D$ sta diminuendo, aumentiamo $\lambda$; (2) se $\overline{E}$ sta aumentando e $\overline{D}$ non decresce, caliamo $\lambda$.
  
  Tabella 1
  \textbf{GOAL:} creare geneticamente un ensemble di reti accurate e diversificate.
  \begin{itemize}
   \item Creare e addestrare la popolazione iniziale di reti.
   \item Finch\'e non si raggiunge un limite di efficacia o di tempo:
   \begin{itemize}
    \item Utilizzare operazioni genetiche per creare nuove reti.
    \item Addestrare le nuove reti utilizzando l'equazione \ref{EnsembleCostFunc}.
    \item Misurare la diversit\'a di ogni rete rispetto alla popolazione corrente (equazione \ref{diversity}).
    \item Normalizzare i termini di accuratezza e diversit\'a delle singole reti.
    \item Cacolare la funzione di fitness in equazione \ref{fitness}.
    \item Aggiornare la popolazione utilizzando le $N$ reti con valore maggiore per la \ref{fitness}.
    \item Corregere $\lambda$ dell'equazione \ref{fitness}
    \item Utilizzare la popolazione attuale di reti come ensemble e combinare gli output delle reti seguendo l'equazione \ref{JudgeWeights}
   \end{itemize}
  \end{itemize}
  Ribadiamo che una rete utile all'ensemble \'e quella che classifica correttamente i maggior numero possibile di casi, mentre pecca principalmente laddove le altre reti classificano correttamente. Ci preoccupiamo di questo aspetto durante la backpropagation moltiplicando la normale funzione di costo per un termine che misura l'errore combinato della popolazione su un determinato esempio:
  \begin{equation}
   Cost = \sum_{k \in T} \left| \frac{t \left(k\right) - \hat{o} \left(k\right)}{\hat{E}} \right|^{\frac{\lambda}{\lambda + 1}} \left[ t\left(k\right) - a\left(k\right) \right]^2 \label{EnsembleCostFunc}
  \end{equation}
  dove $t(k)$ \'e il target e $a(k)$ \'e l'attivazione della rete per l'esempio k nel training set $T$. Si noti che, dato che la nostra rete non \'e ancora un membro dell'ensemble, $\hat{o} (k)$ e $\hat{E}$ non dipendono dalla nostra rete; il nuovo termine \'e quindi una costante \'e perci\'o una costante quando calcoliamo le derivate nella back propagation. Normalizziamo $t(k) - \hat{o} (k)$ dividendo per l'errore dell'ensemble $\hat{E}$ in modo che il valore medio del nostro nuovo termine sia circa 1 indipendentemente dalla correttezza dell'insieme. Ci\'o \'e particolarmente importante con popolazioni particolarmente accurate, pioch\'e $t(k) - \hat{o} (k)$ sar\'a vicino a 0 per la maggior parte degli esempi e la rete verrebbe addestrata solo su un numero esiguo di esempi. L'esponenete $\frac{\lambda}{\lambda + 1}$ rappresenta il rapporto di importanza del termine di diversit\'a nella funzione di fitness. Ad esempio, se $\lambda$ \'e vicino a 0, la diversit\'a non \'e considereta importante e la rete viene addestrata con la consueta funzione di costo; tuttavia se $\lambda$ \'e grande, la diversit\'a \'e cosiderata importante e il nuovo termine nella funzione di costo assume maggiore importanza. Combiniamo le previsioni delle reti prendendo una somma ponderata dell'output di ciascuna rete, dove ogni peso \'e definito come in precedenza. Riportiamo di seguito i risultati ottenuti da David W. Opitz e Jude W. Shavlik.
  L'algoritmo genetico che utilizziamo per generare nuove topologie di rete \'e l'algoritmo REGENT (Opitz e Shavlik, 1994). REGENT utilizza algoritmi genetici per effettuare ricerche nello spazio delle topologie della rete neurale (KN) basate sulla conoscenza. I KN sono reti le cui topologie sono determinate a seguitp della mappatura diretta di un insieme di regole di base che rappresentano ci\'o che attualmente sappiamo del nostro compito. KBANN (Towell e Shavlik, 1994), per esempio, traduce un insieme di regole proposizionali in una rete neurale, quindi affina i pesi della rete risultante usando la back-propagation. Knn addestrati, come le reti KBANN, hanno dimostrato di generalizzare frequentemente meglio di molte altre tecniche di apprendimento induttivo come le reti neurali standard (Optiz 1995; Towell e Shavlik, 1994). L'uso di KNN ci consente di avere reti altamente corrette nel costro gruppo; tuttavia, poich\'e ogni rete nel nostro insieme \'e inizializzata con lo stesso insieme di regole specifiche del dominio, non ci aspettiamo che vi sia un grande disaccordo tra le reti. Un'alternatia che consideriamo nei nostri esperimenti \'e quella di generare casualmente la nostra opolazione iniziale di topologie di rete, poich\'e a volte le regole specifiche del dominio non sono disponibili. Abbiamo eseguito ADDEMUP sul set di problemi MAX di NYNEX e su tre problemi del progetto genoma umano che aiutano a localizzare i geni nelle sequenze di DNA. Ognuno di questi domini \'e accompagnato da una serie di regole approssimativamente corrette che descrivono ci\'o che attualmente \'e noto sull'attivit\'a (Opitz 1995, Optiz e Shavlik 1994). I nostri esperimenti misurano l'errore del set di test di ADDEMUP su queste attivit\'a. Ogni ensemble \'e composto da 20 reti e gli algoritmi REGENT e ADDEMUP hanno considerato 250 reti durante la loro ricerca genetica. La tabella \ref{NoKnowledgeTab} presenta i risultati del caso in cui gli studenti creano casualmente la topologia delle loro reti. La prima riga della tabella \ref{NoKnowledgeTab}, la migliore rete, deriva da una rete neurale a singolo strato in cui, per ogni fold abbiamo addestrato 20 reti contenenti  tra 0 e 100 hidden nodes e usato un set di validazione per scegliere la migliore. La riga successiva, il bagging, contiene i risultati dell'esecuzione dell'algoritmo di bagging di Breiman (1994) su reti standard con un unico hidden layer, in cui il numero di hidden nodes viene impostato casualmente tra 0 e 100 per ogni rete. Il bagging \'e un ``bootstrap'', cio\'e un metodo dell'ensemble che forma ogni rete dell'ensemble con una diversa partizione dell'insieme di addestramento. Genera ogni partizione tracciando casualmente, con la sostituzione, N esempi dal set di addestramento, dove N \'e la dimensione del set di addestramento. Breiman (1994) ha dimostrato che il bagging \'e efficace su algoritmi di apprendimento ``instabili'', come le reti neurali, in cui piccole cambiamenti nel set di addestramento comportano grandi cambiamenti nelle previsioni. La riga inferiore della tabella \ref{NoKnowledgeTab}, ADDEMUP, contiene i risultati di una serie di ADDEMUP in cui la popolazione iniziale di 20 individui viene generata casualmente. I risultati mostrano che su questi domini la combinazione dell'output di pi\'u reti addestrate di generalizza meglio rispetto al tentativo di scegliere la rete singola rete migliore. Mentre la tabella in altomostra la potenza degli insiemi di reti neurali, la tabella \ref{KnowledgeTab} mostra la capacit\'a di ADDEMUP di utilizzare le conoscenze a priori del problema. La prima riga della tabella \ref{KnowledgeTab} contiene i risultati di generalizzazione dell'algoritmo KBANN, mentre la riga successica, KBANN-bagging, contiene i risultati dell'ensemble in cui ogni singola rete dell'ensemble \'e la rete KBANN addestrata su una diversa partizione dell'insieme di  addestramento. Sebbene ciascuna di queste reti inizi con la stessa topologia e impostazione di peso iniziali ``grande'' (i pesi risultanti dalla conoscenza specifica del dominio), piccoli cambiamenti nel set di allenamento producono ancora cambiamenti significativi nelle previsioni. Si noti inoltre che su tutti i set di dati, il bagging KBANN \'e buono o migliore dell'esecuzione del bagging su reti generate casualmente (bagging della tabella \ref{NoKnowledgeTab}). La riga successiva, REGENT-combined, contiene i risultati della semplice combinazione, usando l'equazione \ref{JudgeWeights}, le reti nella popolazione finale di REGENT. ADDEMUP, l'ultima riga della tabella \ref{KnowledgeTab}, differisce principalmente da REGENT-combined in due modi: (1) la sua funzione di fitness (equazione \ref{fitness} ) tiene conto della diversit\'a piuttosto che della sola precisione della rete e (2) allena le nuove reti enfatizzando gli esempi errati dell'ensemble attuale. Pertanto, il confronto di ADDEMUP con REGENT-combined aiuta a testare direttamente la diversit\'a di ADDEMUP raggiungendo l'euristica, anche se i risultati aggiuntivi riportati in Opitz (1995) mostrano che ADDEMUP ottiene gran parte del suo miglioramento dalla sua funzione di fitness. Ci sono due ragioni principali per cui riteniamo che i risultati du ADDEMUP nella tabella \ref{KnowledgeTab} siano particolarmente incoraggianti: (1) confrontando ADDEMUP con REGENT-combined, testiamo esplicitamente la qualit\'a della nostra euristica e dimostriamo la loro fficacia e (2) ADDEMUP \'e in grado di utilizzare efficacemente le conoscenze di base per ridurre l'errore delle singole reti nel suo insieme, pur essendo in grado di creare abbastanza diversit\'a tra di loro in modo da migliorare la qualit\'a complessiva dell'ensemble. 
  
  \begin{table}[h]\caption{Standard neural networks (no domain-specific knowledge used)} \label{NoKnowledgeTab}
   \centering
   \begin{tabular}[h]{|l|c|c|c|c|}
    \hline
    & Promoters & Splice Junction & RBS & MAX \\ \hline
   best-network & 6.6\% & 7.8\% & 10.7\% & 37.0\% \\ 
   bagging & 4.6\% & 4.5\% & 9.5\% & 35.7\% \\
   ADDEMUP & 4.6\% & 4.9\% & 9.0\% & 34.9\% \\ \hline
   \end{tabular}
  \end{table}
  
  \begin{table}[h]\caption{Knowledge-based neural networks (domain-specific knowledge used)} \label{KnowledgeTab}
   \centering
   \begin{tabular}[h]{|l|c|c|c|c|}
    \hline
       & Promoters & Splice Junction & RBS & MAX \\ \hline
    KBANN & 6.2\% & 5.3\% & 9.4\% & 35.8\% \\ 
    KBANN-bagging & 4.2\% & 4.5\% & 8.5\% & 35.6\% \\
    REGENT-combined & 3.9\% & 3.9\% & 8.2\% & 35.6\% \\
    ADDEMUP & 2.9\% & 3.6\% & 7.5\% & 34.7\% \\ \hline
   \end{tabular}
  \end{table}







 \subsection{Funzione d'attivazione sticastica}
 
 \subsection{regolarizzazione (forse)}
 
 \section{Applicazione delle reti neurali complesse al MRF}
 \subsection{introduzione al MRF}
 
 \subsection{MRF con reti neurali a valori reali}
 
 \subsection{MRF con reti neurali a valori complessi}
 
 
\end{document}
